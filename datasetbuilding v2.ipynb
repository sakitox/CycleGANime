{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1998,
     "status": "ok",
     "timestamp": 1593452012444,
     "user": {
      "displayName": "Akiro Semtei-Rotundo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimF0kENclXhGOVObu8HViQHL1Vp0ylXs9Ti4ZnJUo=s64",
      "userId": "09620896528867545722"
     },
     "user_tz": -60
    },
    "id": "zeCnXUyVMnjW"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.ndimage as ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,GlobalMaxPooling2D,Dropout,Flatten,BatchNormalization,Activation\n",
    "from tensorflow.keras.applications import NASNetLarge,VGG19\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.applications.nasnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array,load_img,ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 524,
     "status": "ok",
     "timestamp": 1593452012445,
     "user": {
      "displayName": "Akiro Semtei-Rotundo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimF0kENclXhGOVObu8HViQHL1Vp0ylXs9Ti4ZnJUo=s64",
      "userId": "09620896528867545722"
     },
     "user_tz": -60
    },
    "id": "7q86ZXseNHlG",
    "outputId": "b6f7ea4f-6ae9-45ff-bf46-098afa1996b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1593452013024,
     "user": {
      "displayName": "Akiro Semtei-Rotundo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimF0kENclXhGOVObu8HViQHL1Vp0ylXs9Ti4ZnJUo=s64",
      "userId": "09620896528867545722"
     },
     "user_tz": -60
    },
    "id": "2nWU48JAMnjY"
   },
   "outputs": [],
   "source": [
    "anime_folder = './Anime/*.jpg'\n",
    "human_folder = './Human/*.png'\n",
    "IMAGE_SIZE = [256, 256]\n",
    "VALIDATION_SPLIT = 0.3\n",
    "batch_size1 = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12416,
     "status": "ok",
     "timestamp": 1593452026540,
     "user": {
      "displayName": "Akiro Semtei-Rotundo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimF0kENclXhGOVObu8HViQHL1Vp0ylXs9Ti4ZnJUo=s64",
      "userId": "09620896528867545722"
     },
     "user_tz": -60
    },
    "id": "FDyVB9mK6jMy",
    "outputId": "962bd25c-0c3b-4aa8-f2d3-212c4134fba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern matches 92257 data files. Splitting dataset into 64580 training files and 27677 validation files\n",
      "With a batch size of 15, there will be 4305 batches per training epoch and 1845 batch(es) per validation run.\n"
     ]
    }
   ],
   "source": [
    "anime = tf.io.gfile.glob(anime_folder)\n",
    "split = int(len(anime) * VALIDATION_SPLIT)\n",
    "training_filenames = anime[split:]\n",
    "validation_filenames = anime[:split]\n",
    "print(\"Pattern matches {} data files. Splitting dataset into {} training files and {} validation files\".format(len(anime), len(training_filenames), len(validation_filenames)))\n",
    "validation_steps = int(len(validation_filenames)/ batch_size1)\n",
    "steps_per_epoch = int(len(training_filenames)/ batch_size1)\n",
    "print(\"With a batch size of {}, there will be {} batches per training epoch and {} batch(es) per validation run.\".format(batch_size1, steps_per_epoch, validation_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern matches 21754 data files. Splitting dataset into 15228 training files and 6526 validation files\n",
      "With a batch size of 15, there will be 1015 batches per training epoch and 435 batch(es) per validation run.\n"
     ]
    }
   ],
   "source": [
    "human = tf.io.gfile.glob(human_folder)\n",
    "split1 = int(len(human) * VALIDATION_SPLIT)\n",
    "training_filenames1 = human[split1:]\n",
    "validation_filenames1 = human[:split1]\n",
    "print(\"Pattern matches {} data files. Splitting dataset into {} training files and {} validation files\".format(len(human), len(training_filenames1), len(validation_filenames1)))\n",
    "validation_steps = int(len(validation_filenames1)/ batch_size1)\n",
    "steps_per_epoch = int(len(training_filenames1)/ batch_size1)\n",
    "print(\"With a batch size of {}, there will be {} batches per training epoch and {} batch(es) per validation run.\".format(batch_size1, steps_per_epoch, validation_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized=[]\n",
    "for i in range(len(human)):\n",
    "    img = cv2.imread(human[i], cv2.IMREAD_UNCHANGED)\n",
    "    resized.append(cv2.resize(img, (256,256), interpolation = cv2.INTER_AREA))\n",
    "    status = cv2.imwrite('./Human_resized/' + str(i) + '.jpg',resized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized=[]\n",
    "for i in range(len(anime)):\n",
    "    img = cv2.imread(anime[i], cv2.IMREAD_UNCHANGED)\n",
    "    resized.append(cv2.resize(img, (256,256), interpolation = cv2.INTER_AREA))\n",
    "    status = cv2.imwrite('./Anime_resized/' + str(i) + '.jpg',resized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_folder_resized = './Human_resized/*.jpg'\n",
    "human_resized = tf.io.gfile.glob(human_folder_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_folder_resized = './Anime_resized/*.jpg'\n",
    "anime_resized = tf.io.gfile.glob(anime_folder_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_human = {}\n",
    "for i in range(len(human_resized)):\n",
    "    dic_human[human_resized[i]]  =  i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_anime = {}\n",
    "for i in range(len(anime_resized)):\n",
    "    dic_anime[anime_resized[i]]  =  i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_example(image_string, label):\n",
    "    image_shape = tf.image.decode_jpeg(image_string).shape\n",
    "    \n",
    "    feature = {\n",
    "      'label': _int64_feature(label),\n",
    "      'image_raw': _bytes_feature(image_string),\n",
    "  }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_file = 'CycleGANime-Anime.tfrecords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_file = 'CycleGANime-Humans.tfrecords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(anime_file) as writer:\n",
    "    for filename, label in dic_anime.items():\n",
    "        image_string = open(filename, 'rb').read()\n",
    "        tf_example = image_example(image_string, label)\n",
    "        writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(human_file) as writer:\n",
    "    for filename, label in dic_human.items():\n",
    "        image_string = open(filename, 'rb').read()\n",
    "        tf_example = image_example(image_string, label)\n",
    "        writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BEHOLDERNETTPU.ipynb",
   "provenance": [
    {
     "file_id": "17oc9VNdVRdXF9CSF9K4TW4mKsAygmuL4",
     "timestamp": 1593432801470
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
